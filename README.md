Este proyecto consiste en reescribir las funciones bÃ¡sicas de PyTorch desde cero, utilizando Ãºnicamente NumPy. El objetivo es aprender cÃ³mo funcionan internamente los tensores, la propagaciÃ³n, y el autograd, entre otros.

ğŸš€ CaracterÃ­sticas

ImplementaciÃ³n de tensores y operaciones bÃ¡sicas (suma, multiplicaciÃ³n, transposiciÃ³n, etc.) âœ…

Autograd: cÃ¡lculo manual del gradiente y backward propagation âœ…

Funciones de activaciÃ³n: ReLU, Sigmoid, Tanh, Softmax. âœ…

Funciones de pÃ©rdida: MSE, Cross-Entropy.

OptimizaciÃ³n con Gradient Descent, Adam, RMSProp.

Ejemplos prÃ¡cticos: regresiÃ³n lineal, clasificaciÃ³n simple.

ğŸ“‚ Estructura del Proyecto<br>
```
MiniTorch/
â”‚
â”œâ”€â”€ src/ # Ejemplos de uso
â”‚ â”œâ”€â”€ tensor.py # Clase Tensor personalizada
â”‚ â”œâ”€â”€ operations.py # Operaciones bÃ¡sicas (suma, multiplicaciÃ³n, etc.)
â”‚ â”œâ”€â”€ activations.py # Funciones de activaciÃ³n
â”‚ â”œâ”€â”€ losses.py # Funciones de pÃ©rdida
â”‚ â””â”€â”€ optimizers.py # Optimizadores (SGD)
â”œâ”€â”€ examples/ # Ejemplos de uso
â”‚ â””â”€â”€ linear_regression.py
â”œâ”€â”€ tests/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
ğŸ“ Ejemplo de Uso


ğŸ“Œ Como instalarlo
