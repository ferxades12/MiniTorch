Este proyecto consiste en reescribir las funciones básicas de PyTorch desde cero, utilizando únicamente NumPy. El objetivo es aprender cómo funcionan internamente los tensores, la propagación, y el autograd, entre otros.

🚀 Características

Implementación de tensores y operaciones básicas (suma, multiplicación, transposición, etc.) ✅

Autograd: cálculo manual del gradiente y backward propagation ✅

Funciones de activación: ReLU, Sigmoid, Tanh, Softmax. ✅

Funciones de pérdida: MSE, Cross-Entropy.

Optimización con Gradient Descent, Adam, RMSProp.

Ejemplos prácticos: regresión lineal, clasificación simple.

📂 Estructura del Proyecto<br>
```
MiniTorch/
│
├── src/ # Ejemplos de uso
│ ├── tensor.py # Clase Tensor personalizada
│ ├── operations.py # Operaciones básicas (suma, multiplicación, etc.)
│ ├── activations.py # Funciones de activación
│ ├── losses.py # Funciones de pérdida
│ └── optimizers.py # Optimizadores (SGD)
├── examples/ # Ejemplos de uso
│ └── linear_regression.py
├── tests/
├── README.md
└── requirements.txt
```
📝 Ejemplo de Uso


📌 Como instalarlo
