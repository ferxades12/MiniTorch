Este proyecto consiste en reescribir las funciones bÃ¡sicas de PyTorch desde cero, utilizando Ãºnicamente NumPy. El objetivo es aprender cÃ³mo funcionan internamente los tensores, la propagaciÃ³n, y el autograd, entre otros.

ğŸš€ CaracterÃ­sticas

ImplementaciÃ³n de tensores y operaciones bÃ¡sicas (suma, multiplicaciÃ³n, transposiciÃ³n, etc.) âœ…

Autograd: cÃ¡lculo manual del gradiente y backward propagation âœ…

Funciones de activaciÃ³n: ReLU, Sigmoid, Tanh.

Funciones de pÃ©rdida: MSE, Cross-Entropy.

OptimizaciÃ³n con Gradient Descent, Adam, RMSProp.

Ejemplos prÃ¡cticos: regresiÃ³n lineal, clasificaciÃ³n simple.

ğŸ“‚ Estructura del Proyecto<br>
MiniTorch/<br>
â”‚<br>
â”œâ”€â”€ src/           # Ejemplos de uso<br>
â”‚   â””â”€â”€ tensor.py       # Clase Tensor personalizada  <br>
â”‚   â””â”€â”€ operations.py       # Operaciones bÃ¡sicas (suma, multiplicaciÃ³n, etc.)  <br>
â”‚   â””â”€â”€ activations.py      # Funciones de activaciÃ³n<br>
â”‚   â””â”€â”€ losses.py           # Funciones de pÃ©rdida<br>
â”‚   â””â”€â”€ optimizers.py       # Optimizadores (SGD)<br>
â”œâ”€â”€ examples/           # Ejemplos de uso<br>
â”‚   â””â”€â”€ linear_regression.py<br>
â”œâ”€â”€ tests/    <br>
â””â”€â”€ README.md<br>
â””â”€â”€ requirements.txt


ğŸ“ Ejemplo de Uso


ğŸ“Œ Como instalarlo
