PyTorch From Scratch with NumPy ğŸâ•ğŸ”¢

Este proyecto consiste en reescribir las funciones bÃ¡sicas de PyTorch desde cero, utilizando Ãºnicamente NumPy. El objetivo es aprender cÃ³mo funcionan internamente los tensores, la propagaciÃ³n, y la autograd.

ğŸš€ CaracterÃ­sticas

ImplementaciÃ³n de tensores y operaciones bÃ¡sicas (suma, multiplicaciÃ³n, transposiciÃ³n, etc.).

Funciones de activaciÃ³n: ReLU, Sigmoid, Tanh.

Funciones de pÃ©rdida: MSE, Cross-Entropy.

Autograd: cÃ¡lculo manual del gradiente y backward propagation.

OptimizaciÃ³n con Gradient Descent, Adam, RMSProp.

Ejemplos prÃ¡cticos: regresiÃ³n lineal, clasificaciÃ³n simple.

ğŸ“‚ Estructura del Proyecto<br>
MiniTorch/<br>
â”‚<br>
â”œâ”€â”€ tensor.py           # Clase Tensor personalizada  <br>
â”œâ”€â”€ operations.py       # Operaciones bÃ¡sicas (suma, multiplicaciÃ³n, etc.)  <br>
â”œâ”€â”€ activations.py      # Funciones de activaciÃ³n<br>
â”œâ”€â”€ losses.py           # Funciones de pÃ©rdida<br>
â”œâ”€â”€ optimizers.py       # OptimizaciÃ³n (SGD)<br>
â”œâ”€â”€ examples/           # Ejemplos de uso<br>
â”‚   â””â”€â”€ linear_regression.py<br>
â”œâ”€â”€ tests/    <br>
â””â”€â”€ README.md<br>


ğŸ“ Ejemplo de Uso


ğŸ’¡ MotivaciÃ³n


ğŸ“Œ PrÃ³ximos pasos
Soporte para mÃ¡s funciones de activaciÃ³n y pÃ©rdidas.

ImplementaciÃ³n de redes neuronales completas (MLP, CNN bÃ¡sicas).
