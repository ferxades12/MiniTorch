This project consist of implementing Pytorch basic functionality, usin only NumPy (the other libs are for /test module)
The objective is to learn how neural networks work internally

## ðŸš€ Roadmap

- Tensor implementation and basic operations (sum, mul, transpose, etc.) âœ…
- Autograd: manual grad calculation and backward propagation âœ…
- Activation functions: ReLU, Sigmoid, Tanh, Softmax âœ…
- Loss functions: MSE, Cross-Entropy âœ…
- Optimizers: Gradient Descent, Adam âœ…
- Neural network layers: Linear, Sequential, RNN âœ…
- Regularizations: L1, L2, dropout âœ…
- Datasets, Dataloaders, random_split âœ…
- Practical examples: xor classification, RNN âœ…
- Meta + Kernel architecture (preparation for Rust/CUDA) âœ…
- Model serialization 
- ML Algorithms 

## ðŸš¬:

- Meta + kernel design (preparation for Rust) âœ…
- CUDA integration (CuPy) 
- Rust backend 
- Rust parallelism 
- CUDA integration in Rust 



## ðŸ“‚ Project Structure<br>
```
MiniTorch/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ops/                   
â”‚ â”‚ â”œâ”€â”€ autograd.py          # Operation classes with forward/backward
â”‚ â”‚ â”œâ”€â”€ dispatch.py          # Dispatch system and meta logic
â”‚ â”‚ â”œâ”€â”€ cpu.py               # Pure CPU kernels (NumPy)
â”‚ â”‚ â””â”€â”€ cuda.py              # GPU kernels
â”‚ â”œâ”€â”€ nn/
â”‚ â”‚ â”œâ”€â”€ activations.py       # Activation functions (ReLU, Sigmoid, Tanh, Softmax)
â”‚ â”‚ â”œâ”€â”€ losses.py            # Loss functions (MSE, CrossEntropy)
â”‚ â”‚ â”œâ”€â”€ functional.py        # Non-class functions (wrappers)
â”‚ â”‚ â”œâ”€â”€ layers.py            # Network layers (Linear, Sequential, RNN, Dropout)
â”‚ â”‚ â”œâ”€â”€ regularizations.py   # L1, L2
â”‚ â”‚ â”œâ”€â”€ optimizers.py        # SGD, Adam
â”‚ â”‚ â””â”€â”€ module.py            # Module base class
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â””â”€â”€ data.py              # Dataset, DataLoader, random_split
â”‚ â”œâ”€â”€ base.py                # Function base class for autograd
â”‚ â””â”€â”€ tensor.py              # Tensor class with autograd
â”œâ”€â”€ examples/                 # Usage examples
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```


## Installation

Clone the repository:
```bash
git clone https://github.com/ferxades12/MiniTorch
cd MiniTorch
```

Create virtual environment (optional but recommended):
```bash
python -m venv venv
# On Linux/macOS
source venv/bin/activate
# On Windows
venv\Scripts\activate
```

Install dependencies:
```bash
pip install -r requirements.txt
```

## GPU Support (CUDA)

If you want to use GPU acceleration with CUDA, you'll need to install CuPy separately along with NVIDIA CUDA Toolkit. CuPy is not included in requirements.txt as it requires specific NVIDIA drivers and CUDA versions. Visit [CuPy installation guide](https://docs.cupy.dev/en/stable/install.html) for detailed instructions.

```bash
# Example for CUDA 13.x
pip install cupy-cuda13x
```
