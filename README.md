Este proyecto consiste en reescribir las funciones básicas de PyTorch desde cero, utilizando únicamente NumPy. El objetivo es aprender cómo funcionan internamente los tensores, la propagación, y el autograd, entre otros.

🚀 Características

Implementación de tensores y operaciones básicas (suma, multiplicación, transposición, etc.) ✅

Autograd: cálculo manual del gradiente y backward propagation ✅

Funciones de activación: ReLU, Sigmoid, Tanh.

Funciones de pérdida: MSE, Cross-Entropy.

Optimización con Gradient Descent, Adam, RMSProp.

Ejemplos prácticos: regresión lineal, clasificación simple.

📂 Estructura del Proyecto<br>
MiniTorch/<br>
│<br>
├── src/           # Ejemplos de uso<br>
│   └── tensor.py       # Clase Tensor personalizada  <br>
│   └── operations.py       # Operaciones básicas (suma, multiplicación, etc.)  <br>
│   └── activations.py      # Funciones de activación<br>
│   └── losses.py           # Funciones de pérdida<br>
│   └── optimizers.py       # Optimizadores (SGD)<br>
├── examples/           # Ejemplos de uso<br>
│   └── linear_regression.py<br>
├── tests/    <br>
└── README.md<br>
└── requirements.txt


📝 Ejemplo de Uso


📌 Como instalarlo
