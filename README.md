PyTorch From Scratch with NumPy ğŸâ•ğŸ”¢

Este proyecto consiste en reescribir las funciones bÃ¡sicas de PyTorch desde cero, utilizando Ãºnicamente NumPy. El objetivo es aprender cÃ³mo funcionan internamente los tensores, la propagaciÃ³n, y la autograd.

ğŸš€ CaracterÃ­sticas

ImplementaciÃ³n de tensores y operaciones bÃ¡sicas (suma, multiplicaciÃ³n, transposiciÃ³n, etc.).

Funciones de activaciÃ³n: ReLU, Sigmoid, Tanh.

Funciones de pÃ©rdida: MSE, Cross-Entropy.

Autograd: cÃ¡lculo manual del gradiente y backward propagation.

OptimizaciÃ³n con Gradient Descent, Adam, RMSProp.

Ejemplos prÃ¡cticos: regresiÃ³n lineal, clasificaciÃ³n simple.

ğŸ“‚ Estructura del Proyecto
pytorch_from_scratch_numpy/
â”‚
â”œâ”€â”€ tensor.py           # Clase Tensor personalizada
â”œâ”€â”€ operations.py       # Operaciones bÃ¡sicas (suma, multiplicaciÃ³n, etc.)
â”œâ”€â”€ activations.py      # Funciones de activaciÃ³n
â”œâ”€â”€ losses.py           # Funciones de pÃ©rdida
â”œâ”€â”€ optimizers.py       # OptimizaciÃ³n (SGD)
â”œâ”€â”€ examples/           # Ejemplos de uso
â”‚   â””â”€â”€ linear_regression.py
â”œâ”€â”€ tests/    
â””â”€â”€ README.md


ğŸ“ Ejemplo de Uso


ğŸ’¡ MotivaciÃ³n


ğŸ“Œ PrÃ³ximos pasos
Soporte para mÃ¡s funciones de activaciÃ³n y pÃ©rdidas.

ImplementaciÃ³n de redes neuronales completas (MLP, CNN bÃ¡sicas).
